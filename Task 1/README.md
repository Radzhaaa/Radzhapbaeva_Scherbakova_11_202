# Домашка 1 — краулер

По заданию надо было скачать минимум 100 текстовых страниц по списку ссылок. Я сделала так: читаю ссылки из urls.txt, качаю каждую страницу и сохраняю в папку pages в виде текстовых файлов (html не убираю, как в задании). Ещё пишу index.txt — там номер файла и ссылка на страницу.

Как запустить:
1. В urls.txt должен быть список ссылок, по одной на строку. Если файла нет — я добавила get_urls.py, он собирает ссылки на статьи с википедии (русские).
2. Открыть папку Task 1 в терминале и выполнить: python crawler.py
3. Файлы появятся в pages/, index.txt создастся там же.

Нужен питон 3.

На сдачу (как в задании): ссылка на этот репо с кодом краулера; отдельно архив, в котором лежат папка pages (все выкачанные страницы) и файл index.txt.

Чуть подробнее про запуск — в DEPLOYMENT.md. Что сделано — в RELEASE_NOTES.md.
