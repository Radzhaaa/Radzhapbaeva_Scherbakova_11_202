# Домашка 1 — краулер

По заданию надо было скачать минимум 100 текстовых страниц по списку ссылок. Я сделала так: читаю ссылки из urls.txt, качаю каждую страницу и сохраняю в папку pages в виде текстовых файлов (html не убираю, как в задании). Ещё пишу index.txt — там номер файла и ссылка на страницу.

Как запустить:
1. В urls.txt должен быть список ссылок, по одной на строку. Если файла нет — я добавила get_urls.py, он собирает ссылки на статьи с википедии (русские).
2. Открыть папку Task 1 в терминале и выполнить: python crawler.py
3. Файлы появятся в pages/, index.txt создастся там же.

Нужен питон 3.

На сдачу (как в задании): ссылка на этот репо с кодом краулера; отдельно архив, в котором лежат папка pages (все выкачанные страницы) и файл index.txt.

Чуть подробнее про запуск — в DEPLOYMENT.md. Что сделано — в RELEASE_NOTES.md.

# Домашка 2 - токенизация и лемматизация

По заданию нужно было из сохраненных документов выделить отдельные слова (токенизация) и получить список токенов

При этом список не должен содержать дубликатов, союзов, предлогов, чисел и список не должен содержать "мусора" (слов содержащих одновременно буквы и цифры, обрывки разметки и тд.)

Так же требовалось привести слова к нормальной форме (лемматизация) и сохранить результат в виде словаря  <лемма><пробел><токен 1><пробел><токен 2>.....<пробел><токенN><\n>

Для запуска:
1. Убедиться что в папке Task1 есть распакованный архив с результатами первой домашки (папка pages)
2. Открыть папку Task2 в терминале и выполнить: pyton process.py
3. Результат будет сохранён в файлах lemmas.txt и tokens.txt в папке Task2.

На сдачу (как в задании): ссылка на этот репо с кодом лумматизации и токенизации; отдельно файлы lemmas.txt и tokens.txt.