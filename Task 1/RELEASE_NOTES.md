# Release Notes — задание 1

Что сделала: краулер по списку ссылок из urls.txt. Качает страницы, сохраняет в текстовые файлы в папку pages (html разметку не убираю, как в задании). Пишет index.txt — номер файла и ссылка.

get_urls.py — я его делала чтобы быстро набрать 100 ссылок с википедии, можно и свой urls.txt написать вручную.
crawler.py — основной скрипт. Читает urls.txt, качает только страницы с text/html (без картинок и т.п.), сохраняет как page_001.txt, page_002.txt и так далее. Между запросами небольшая пауза чтобы не перегружать.

На сдачу: ссылка на репо + архив, в котором папка pages и index.txt.
